# -*- coding: utf-8 -*-
"""FaceEmotionRecog_with_FER2013_AUG_Data_Accurate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16uHwCGxgA82998Ap7r7XT1iOOwfUuqoL
"""

# !gdown --id 182kcQznm2GlXW1Um29pw3nlnNY0xu-nz
# !mkdir data
# !unzip '/content/data_with_accurate_data(output).zip' -d '/content/data/'
# !rm -r data_with_accurate_data(output).zip



import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#import matplotlib as plt
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Reshape, Dropout, BatchNormalization
from tensorflow.keras.preprocessing import image
import numpy as np
import json
import os
# import albumentations as alb
import time
import cv2
from tensorflow.keras.applications import ResNet152V2, VGG16

from tensorflow.keras.callbacks import EarlyStopping
from keras.preprocessing.image import img_to_array

import albumentations as alb

gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
  tf.config.experimental.set_memory_growth(gpu, True)


print("GPU count ---------------------------------->>> ", gpus)

def load_image(x):
    byte_img = tf.io.read_file(x)
    img = tf.io.decode_jpeg(byte_img)
    return img

# for testing
images1 = tf.data.Dataset.list_files('data/FER2013/with_augmented/content/data/aug/train/happy/*.jpg', shuffle=False)
images1 = images1.map(load_image)
plt.imshow(images1.as_numpy_iterator().next())

# for testing
image_generator1 = images1.batch(4).as_numpy_iterator()
plot_images1 = image_generator1.next()
fig, ax = plt.subplots(ncols=4, figsize=(20,20))
for idx, image1 in enumerate(plot_images1):
    ax[idx].imshow(image1)
plt.show()





data_gen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, rescale=1./255, validation_split=0.2)
path_to_data = 'data/FER2013/with_augmented/content/data/aug/train'

training_set = data_gen.flow_from_directory(path_to_data,(48,48),color_mode='grayscale', subset="training")
testing_set = data_gen.flow_from_directory(path_to_data,(48,48),color_mode='grayscale', subset="validation")

113968/32

# Get the class indices (mapping of class names to indices)
class_indices = training_set.class_indices
print("Class Indices:", class_indices)

# Get the class labels for all images
class_labels = training_set.classes

# Count the number of images in each class
unique, counts = np.unique(class_labels, return_counts=True)
class_distribution = dict(zip(unique, counts))

# Map class indices back to class names
class_names = {v: k for k, v in class_indices.items()}
class_distribution_named = {class_names[k]: v for k, v in class_distribution.items()}

# Print the number of images in each class
print("Number of images in each class (by index):", class_distribution)
print("Number of images in each class (by name):", class_distribution_named)

training_set.shape

print(training_set.class_indices)
print(training_set.batch_size)
labels = ["angry","disgust","fear","happy","neutral","sad","surprise"]

def plotImages(image_arr):
  fig, axes = plt.subplots(1, 6, figsize=(20,20))
  for img, ax in zip(image_arr, axes):
    ax.imshow(img)
    ax.axis('off')
  plt.tight_layout()
  plt.show()
training_images, _ = next(training_set)
plotImages(training_images[:6])

for i in range(len(labels)):
    print(labels[i] + ': ' + str(np.count_nonzero(labels[np.argmax(res[1][idx])] == i)))

next(training_set)[0][1].shape

next(training_set)[1][1].shape

next(training_set)[1][1]

print(labels[np.argmax(kk[1][1])])
plt.imshow(kk[0][1])

print(labels[np.argmax(kk[1][1])])

#test
# data_samples2 = next(training_set)
res = next(training_set)

fig, ax = plt.subplots(ncols=4, figsize=(20,20))
for idx in range(4):
  sample_imagess = res[0][idx].copy()
  sample_emotion = labels[np.argmax(res[1][idx])]

  position = (10, 20)  # (x, y) coordinates
  font = cv2.FONT_HERSHEY_SIMPLEX
  font_scale = 0.5
  color = (0, 0, 0)  # Black color in BGR
  thickness = 1

  cv2.putText(sample_imagess, sample_emotion, position, font, font_scale, color, thickness)

  ax[idx].imshow(sample_imagess)


from sklearn.utils.class_weight import compute_class_weight

# Assuming you have the class labels for your training data
# Replace `training_set.classes` with the array of class labels from your dataset
class_labels = training_set.classes  # This contains the class indices for all images

# Get the unique class indices and their corresponding weights
class_weights = compute_class_weight(
    class_weight='balanced',  # Automatically balance weights
    classes=np.unique(class_labels),  # Unique class indices
    y=class_labels  # Class labels for all samples
)

# Convert the result to a dictionary
class_weights_dict = dict(enumerate(class_weights))

print("Class Weights:", class_weights_dict)

#######################################         ----------------------- MODEL 5 ------------------------------------------
import keras

early = EarlyStopping(monitor='val_loss', patience=5)

face_recognition_model_5 = keras.Sequential()

face_recognition_model_5.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1), padding='same',kernel_regularizer='l2'))
face_recognition_model_5.add(Conv2D(64, (3, 3), activation='relu',kernel_regularizer='l2'))
face_recognition_model_5.add(BatchNormalization())
face_recognition_model_5.add(MaxPooling2D(pool_size=(2, 2))) #max pooling to decrease dimension
face_recognition_model_5.add(Dropout(0.25)) #test

face_recognition_model_5.add(Conv2D(128, (3, 3), activation='relu',kernel_regularizer='l2'))
face_recognition_model_5.add(BatchNormalization())
face_recognition_model_5.add(MaxPooling2D(pool_size=(2, 2))) #max pooling to decrease dimension
face_recognition_model_5.add(Conv2D(128, (3, 3), activation='relu',kernel_regularizer='l2'))
face_recognition_model_5.add(BatchNormalization())
face_recognition_model_5.add(Dropout(0.25))

face_recognition_model_5.add(Flatten())

face_recognition_model_5.add(Dense(1024, activation = 'relu',kernel_regularizer='l2'))
face_recognition_model_5.add(Dropout(0.5))
face_recognition_model_5.add(Dense(7, activation = 'softmax'))


# compile
face_recognition_model_5.compile(optimizer=keras.optimizers.SGD(learning_rate=0.001),loss=keras.losses.CategoricalCrossentropy(),metrics=['acc'])

early = EarlyStopping(monitor='val_loss',patience=10)

face_recognition_model_5.summary()

history_new = face_recognition_model_5.fit(
    training_set,
    epochs=1,
    validation_data=testing_set,
    batch_size=32,
    steps_per_epoch=14246,
    validation_steps=3561, #validation_samples // batch_size
    callbacks=[early],
    class_weight=class_weights_dict  # Pass the class weights here
    )

# Retrieve a list of accuracy results on training and validation data
# sets for each training epoch
acc = history_new.history['acc']
val_acc = history_new.history['val_acc']

# Retrieve a list of list results on training and validation data
# sets for each training epoch
loss = history_new.history['loss']
val_loss = history_new.history['val_loss']

# Get number of epochs
# using the minimum length to ensure both lists have the same length
epochs = range(min(len(acc), len(val_acc)))

# Plot training and validation accuracy per epoch
plt.plot(epochs, acc[:len(epochs)]) # Slice acc to match epochs length
plt.plot(epochs, val_acc)
plt.title('Training and validation accuracy')

plt.figure()

# Plot training and validation loss per epoch
plt.plot(epochs, loss[:len(epochs)]) # Slice loss to match epochs length
plt.plot(epochs, val_loss)
plt.title('Training and validation loss')

face_recognition_model_5.save('/content/face_emotion_tracker_with_weight.h5')
