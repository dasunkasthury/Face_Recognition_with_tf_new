# -*- coding: utf-8 -*-
"""FaceEmotionRecog_with_FER2013_AUG_Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16uHwCGxgA82998Ap7r7XT1iOOwfUuqoL
"""

# !gdown --id 1vb6EwFEZKSxwsTNjVIwftEcQnbC20g4c
# !mkdir data
# !unzip '/content/file.zip' -d '/content/data/'
# !rm -r file.zip.zip

import tensorflow as tf
from tensorflow.keras.preprocessing.image import ImageDataGenerator
#import matplotlib as plt
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Input, Reshape, Dropout, BatchNormalization
from tensorflow.keras.preprocessing import image
import numpy as np
import json
import os
import albumentations as alb
import time
import cv2
from tensorflow.keras.applications import ResNet152V2, VGG16

from tensorflow.keras.callbacks import EarlyStopping
from keras.preprocessing.image import img_to_array

def load_image(x):
    byte_img = tf.io.read_file(x)
    img = tf.io.decode_jpeg(byte_img)
    return img





gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
  tf.config.experimental.set_memory_growth(gpu, True)


print("GPU count ---------------------------------->>> ", gpus)

# for testing
# images1 = tf.data.Dataset.list_files('data/FER2013/with_augmented/content/data/aug/train/sad/*.jpg', shuffle=False)
# images1 = images1.map(load_image)
# plt.imshow(images1.as_numpy_iterator().next())

# # for testing
# image_generator1 = images1.batch(4).as_numpy_iterator()
# plot_images1 = image_generator1.next()
# fig, ax = plt.subplots(ncols=4, figsize=(20,20))
# for idx, image1 in enumerate(plot_images1):
#     ax[idx].imshow(image1)
# plt.show()




data_gen = ImageDataGenerator(shear_range=0.2, zoom_range=0.2, rescale=1./255, validation_split=0.2)
path_to_data = 'data/FER2013/with_augmented/content/data/aug/train'

training_set = data_gen.flow_from_directory(path_to_data,(48,48),color_mode='grayscale', subset="training")
testing_set = data_gen.flow_from_directory(path_to_data,(48,48),color_mode='grayscale', subset="validation")

print(training_set.class_indices)
print(training_set.batch_size)
labels = ["angry","disgust","fear","happy","neutral","sad","surprise"]

# def plotImages(image_arr):
#   fig, axes = plt.subplots(1, 6, figsize=(20,20))
#   for img, ax in zip(image_arr, axes):
#     ax.imshow(img)
#     ax.axis('off')
#   plt.tight_layout()
#   plt.show()
# training_images, _ = next(training_set)
# plotImages(training_images[:6])

# kk = next(training_set)

# next(training_set)[0][1].shape

# next(training_set)[1][1].shape

# next(training_set)[1][1]

# print(labels[np.argmax(kk[1][1])])
# plt.imshow(kk[0][1])

# print(labels[np.argmax(kk[1][1])])

#test
# data_samples2 = next(training_set)

# res = next(training_set)

# fig, ax = plt.subplots(ncols=4, figsize=(20,20))
# for idx in range(4):
#   sample_imagess = res[0][idx].copy()
#   sample_emotion = labels[np.argmax(res[1][idx])]

#   position = (10, 20)  # (x, y) coordinates
#   font = cv2.FONT_HERSHEY_SIMPLEX
#   font_scale = 0.5
#   color = (0, 0, 0)  # Black color in BGR
#   thickness = 1

#   cv2.putText(sample_imagess, sample_emotion, position, font, font_scale, color, thickness)

#   ax[idx].imshow(sample_imagess)

# vgg = VGG16(include_top=False) # originally it used ssd the move to vgg16

# #######################################         ----------------------- MODEL 1 ------------------------------------------
# # initialising the CNN
# face_recognition_model = Sequential()

# # convolution to extract features from images
# face_recognition_model.add(Conv2D(32, (3, 3), input_shape = (128, 128, 1), activation = 'relu'))

# # max pooling to get max / largest values in feature map
# # down sampling technique to get the most present features
# face_recognition_model.add(MaxPooling2D(pool_size = (2, 2)))

# # more convolution and max pooling layers
# face_recognition_model.add(Conv2D(64, (3, 3), activation = 'relu'))
# face_recognition_model.add(MaxPooling2D(pool_size = (2, 2)))
# face_recognition_model.add(Conv2D(128, (3, 3), activation = 'relu'))
# face_recognition_model.add(MaxPooling2D(pool_size = (2, 2)))
# face_recognition_model.add(Conv2D(256, (3, 3), activation = 'relu'))
# face_recognition_model.add(MaxPooling2D(pool_size = (2, 2)))

# # flattening is converting the data into a 1-dimensional array
# face_recognition_model.add(Flatten())
# face_recognition_model.add(Dense(units = 1024, activation = 'relu'))
# face_recognition_model.add(Dense(units = 7, activation = 'softmax'))

# # compiling the CNN
# face_recognition_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# # show summary of the created model
# face_recognition_model.summary()

# #######################################         ----------------------- MODEL 2 ------------------------------------------

# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(128, 128, 3))

# # Modify the input layer to accept grayscale images
# input_layer = Input(shape=(128, 128, 1))
# x = tf.keras.layers.Concatenate()([input_layer, input_layer, input_layer])  # Convert 1 channel to 3

# # Pass the input through the base model
# x = base_model(x, training=False)

# # Add custom layers on top
# x = Flatten()(x)
# x = Dense(256, activation='relu')(x)
# output_layer = Dense(7, activation='softmax')(x)

# # Create the model
# model = Model(inputs=input_layer, outputs=output_layer)

# # Compile the model
# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# # Summary of the model
# model.summary()

# layer1 = Conv2D(256, (3, 3), padding='same', activation='relu', input_shape=(128, 128, 1))
# layer2 = BatchNormalization()

# layer2

#######################################         ----------------------- MODEL 3 ------------------------------------------

# # initialising the CNN
# face_recognition_model_2 = Sequential()

# # convolution to extract features from images
# # conv1
# face_recognition_model_2.add(Conv2D(256, (3, 3), padding='same', activation='relu', input_shape=(128, 128, 1)))
# face_recognition_model_2.add(BatchNormalization())

# # conv2
# face_recognition_model_2.add(Conv2D(256, (3, 3), padding='same', activation='relu'))
# face_recognition_model_2.add(BatchNormalization())
# face_recognition_model_2.add(MaxPooling2D((2, 2)))

# # conv3
# face_recognition_model_2.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
# face_recognition_model_2.add(BatchNormalization())


# # conv4
# face_recognition_model_2.add(Conv2D(128, (3, 3), padding='same', activation='relu'))
# face_recognition_model_2.add(BatchNormalization())
# face_recognition_model_2.add(MaxPooling2D((2, 2)))

# # conv5
# face_recognition_model_2.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
# face_recognition_model_2.add(BatchNormalization())

# # conv6
# face_recognition_model_2.add(Conv2D(64, (3, 3), padding='same', activation='relu'))
# face_recognition_model_2.add(BatchNormalization())
# face_recognition_model_2.add(MaxPooling2D((2, 2)))

# # flatten
# face_recognition_model_2.add(Flatten())

# # fc1
# face_recognition_model_2.add(Dense(512, activation='relu'))
# face_recognition_model_2.add(BatchNormalization())

# # fc2
# face_recognition_model_2.add(Dense(256, activation='relu'))
# face_recognition_model_2.add(BatchNormalization())

# # fc3
# face_recognition_model_2.add(Dense(128, activation='relu'))
# face_recognition_model_2.add(BatchNormalization())

# # out
# face_recognition_model_2.add(Dense(7, activation='softmax'))


# face_recognition_model_2.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])

# # show summary of the created model
# face_recognition_model_2.summary()

# #######################################         ----------------------- MODEL 4 ------------------------------------------

# face_recognition_model_3 = tf.keras.models.Sequential([
#     # 1st conv
#   tf.keras.layers.Conv2D(96, (11,11),strides=(4,4), activation='relu', input_shape=(48, 48, 1)),
#   tf.keras.layers.BatchNormalization(),
#   tf.keras.layers.MaxPooling2D(2, strides=(2,2)),
#     # 2nd conv
#   tf.keras.layers.Conv2D(256, (11,11),strides=(1,1), activation='relu',padding="same"),
#   tf.keras.layers.BatchNormalization(),
#      # 3rd conv
#   tf.keras.layers.Conv2D(384, (3,3),strides=(1,1), activation='relu',padding="same"),
#   tf.keras.layers.BatchNormalization(),
#     # 4th conv
#   tf.keras.layers.Conv2D(384, (3,3),strides=(1,1), activation='relu',padding="same"),
#   tf.keras.layers.BatchNormalization(),
#     # 5th Conv
#   tf.keras.layers.Conv2D(256, (3, 3), strides=(1, 1), activation='relu',padding="same"),
#   tf.keras.layers.BatchNormalization(),
#   tf.keras.layers.MaxPooling2D(2, strides=(2, 2)),
#   # To Flatten layer
#   tf.keras.layers.Flatten(),
#   # To FC layer 1
#   tf.keras.layers.Dense(4096, activation='relu'),
#     # add dropout 0.5 ==> tf.keras.layers.Dropout(0.5),
#   #To FC layer 2
#   tf.keras.layers.Dense(4096, activation='relu'),
#     # add dropout 0.5 ==> tf.keras.layers.Dropout(0.5),
#   tf.keras.layers.Dense(7, activation='sigmoid')
# ])



# face_recognition_model_3.compile(optimizer='adam', loss="categorical_crossentropy", metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall(), tf.keras.metrics.SensitivityAtSpecificity(0.5), tf.keras.metrics.SpecificityAtSensitivity(0.5), tf.keras.metrics.AUC(curve='ROC')])

# Summarizing the model architecture and printing it out
# face_recognition_model_3.summary()

# early = EarlyStopping(monitor='val_loss',patience=10)

# logdir='logs'
# tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir) # fit is going to trigger def train_step(self, batch, **kwargs):

# history = face_recognition_model_3.fit(training_set, epochs=150, batch_size=64, validation_data=testing_set,steps_per_epoch=300, callbacks=[early])

# hist = face_recognition_model_3.fit(training_set, epochs=100, batch_size=64, validation_data=testing_set, callbacks=[tensorboard_callback])

# hist = face_recognition_model_5.fit(training_set,epochs=250,batch_size=64,validation_data=testing_set,steps_per_epoch=300,callbacks=[early])

#######################################         ----------------------- MODEL 5 ------------------------------------------
import keras

early = EarlyStopping(monitor='val_loss', patience=5)

face_recognition_model_5 = keras.Sequential()

face_recognition_model_5.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1), padding='same',kernel_regularizer='l2'))
face_recognition_model_5.add(Conv2D(64, (3, 3), activation='relu',kernel_regularizer='l2'))
face_recognition_model_5.add(BatchNormalization())
face_recognition_model_5.add(MaxPooling2D(pool_size=(2, 2))) #max pooling to decrease dimension
face_recognition_model_5.add(Dropout(0.25)) #test

face_recognition_model_5.add(Conv2D(128, (3, 3), activation='relu',kernel_regularizer='l2'))
face_recognition_model_5.add(BatchNormalization())
face_recognition_model_5.add(MaxPooling2D(pool_size=(2, 2))) #max pooling to decrease dimension
face_recognition_model_5.add(Conv2D(128, (3, 3), activation='relu',kernel_regularizer='l2'))
face_recognition_model_5.add(BatchNormalization())
face_recognition_model_5.add(Dropout(0.25))

face_recognition_model_5.add(Flatten())

face_recognition_model_5.add(Dense(1024, activation = 'relu',kernel_regularizer='l2'))
face_recognition_model_5.add(Dropout(0.5))
face_recognition_model_5.add(Dense(7, activation = 'softmax'))


# compile
face_recognition_model_5.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001),loss=keras.losses.CategoricalCrossentropy(),metrics=['acc'])

early = EarlyStopping(monitor='val_loss',patience=10)

face_recognition_model_5.summary()

from sklearn.utils.class_weight import compute_class_weight

# Assuming you have the class labels for your training data
# Replace `training_set.classes` with the array of class labels from your dataset
class_labels = training_set.classes  # This contains the class indices for all images

# Get the unique class indices and their corresponding weights
class_weights = compute_class_weight(
    class_weight='balanced',  # Automatically balance weights
    classes=np.unique(class_labels),  # Unique class indices
    y=class_labels  # Class labels for all samples
)

# Convert the result to a dictionary
class_weights_dict = dict(enumerate(class_weights))

print("Class Weights:", class_weights_dict)

history_new = face_recognition_model_5.fit(
    training_set, 
    epochs=1, 
    validation_data=testing_set, 
    batch_size=32,
    steps_per_epoch=training_set.samples // 32,
    validation_steps=testing_set.samples // 32,
    callbacks=[early],
    class_weight=class_weights_dict  
    )

face_recognition_model_5.save('face_recognition_model_5_80E_64BS_4SPE_650VS.h5')

def plot_accuracy(history):
    plt.plot(history.history["acc"], color='red')
    plt.plot(history.history["val_acc"], color='orange')

    plt.title("Accuracy")

    plt.legend(["acc","val_acc"], bbox_to_anchor =(0.65, 1.00))
    plt.show()

def plot_loss(history):
    plt.plot(history.history["loss"], color='blue')
    # plt.plot(history.history["loss"],color='green')
    plt.plot(history.history["val_loss"],color='olive')
    # plt.plot(history.history["recall_1"],color='violet')
    # plt.plot(history.history["specificity_at_sensitivity_1"],color='purple')

    # plt.plot(history.history["val_accuracy"],color='cyan')
    # plt.plot(history.history["val_auc_1"],color='yellow')

    plt.title("Loss")

    plt.legend(["loss", "val_loss" ], bbox_to_anchor =(0.65, 1.00))
    plt.show()

plot_accuracy(history_new)
plot_loss(history_new)